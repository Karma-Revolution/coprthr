/* vcore.c
 *
 * Copyright (c) 2009-2010 Brown Deer Technology, LLC.  All Rights Reserved.
 *
 * This software was developed by Brown Deer Technology, LLC.
 * For more information contact info@browndeertechnology.com
 *
 * This program is free software: you can redistribute it and/or modify it
 * under the terms of the GNU Lesser General Public License version 3 (LGPLv3)
 * as published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */

/* DAR */

#define _GNU_SOURCE
#include <sched.h>
#include <unistd.h>
#include <sys/syscall.h>
#include <sys/types.h>

#include <stdlib.h>

//#define XCL_NO_UCONTEXT 	/* used for testing only */

#include <pthread.h>
#include <signal.h>
#ifndef XCL_NO_UCONTEXT
#include <ucontext.h>
#endif

#include "xcl_structs.h"	/* XXX this should be temporary! -DAR */

#include "vcore.h"
#include "cmdcall.h"
#include "util.h"


#define NCPU 2 /* XXX this is a hack for testing, fix it -DAR */


//extern void wrapper_3_xxx( void* p );
extern void* vcore_v[];


//#define VCORE_STACK_MASK (~(VCORE_STACK_SZ-1))

static pthread_t engine_td[VCORE_NE];
static pthread_mutex_t engine_mtx[VCORE_NE];
static pthread_cond_t engine_sig1[VCORE_NE];
static pthread_cond_t engine_sig2[VCORE_NE];
static int engine_ready[VCORE_NE];
static int engine_shutdown[VCORE_NE];
static struct cmdcall_arg* engine_cmd_argp[VCORE_NE];
static struct vcengine_data engine_data[VCORE_NE];
static char* engine_local_mem_base[VCORE_NE];
static char* engine_local_mem_free[VCORE_NE];
static size_t engine_local_mem_sz[VCORE_NE];


//#define __fp() __builtin_frame_address(0)
//#define __getvcdata() (struct vc_data*)(((intptr_t)__fp())&VCORE_STACK_MASK)


#if(0)
/*
 * intrinsics called from kernels
 */

void barrier( int flags )
{
	struct vc_data* data = __getvcdata();
	int vcid = data->vcid;
	DEBUG(__FILE__,__LINE__,"barrier: vcore[%d] flags=%x",vcid,flags);

#ifndef XCL_NO_UCONTEXT
	swapcontext(data->this_uctx, data->next_uctx);
#endif
}

uint get_work_dim() { return((__getvcdata())->workp->tdim); }

size_t get_local_size(uint d) { return((__getvcdata())->workp->ltsz[d]); }
size_t get_local_id(uint d) { return((__getvcdata())->ltid[d]); }

size_t get_num_groups(uint d) { return((__getvcdata())->workp->gsz[d]); }
size_t get_global_size(uint d) { return((__getvcdata())->workp->gtsz[d]); }

size_t get_group_id(uint d) { return((__getvcdata())->workp->gid[d]); }
size_t get_global_id(uint d) { 
	struct vc_data* data = __getvcdata();
	return(data->ltid[d] + data->workp->gtid[d]); 
}
#endif



#if(0)
static void
vcore( void* p )
{
	struct vcengine_data* edata = (struct vcengine_data*)p;
	struct vc_data* data = __getvcdata();
	int vcid = data->vcid;

	DEBUG(__FILE__,__LINE__,"vcore[%d] running\n",vcid);

	++edata->vc_runc;

	/* execute kernel */
	barrier(0);
	barrier(0);

	--edata->vc_runc;

	DEBUG(__FILE__,__LINE__,"vcore[%d] halt\n",vcid);
}
#endif


static void* 
vcengine( void* p )
{
	int i;
	struct vcengine_data* edata = (struct vcengine_data*)p;
	int veid = edata->veid;
#ifndef XCL_NO_UCONTEXT
	ucontext_t vcengine_uctx;
	ucontext_t vc_uctx[VCORE_NC];
#endif
	struct work_struct work;
	char vc_stack_storage[(VCORE_NC+1)*VCORE_STACK_SZ];
	char ve_local_mem[VCORE_LOCAL_MEM_SZ];

	cpu_set_t mask;
	if (sched_getaffinity(0,sizeof(cpu_set_t),&mask)) {
		WARN(__FILE__,__LINE__,"vcengine: sched_getaffinity failed");
	} else {
		CPU_ZERO(&mask);
		CPU_SET(veid%NCPU,&mask);
		pid_t td = syscall(SYS_gettid);
		if (sched_setaffinity(td,sizeof(cpu_set_t),&mask)) {
			WARN(__FILE__,__LINE__,"vcengine: sched_setaffinity failed");
		}
	}



	DEBUG(__FILE__,__LINE__,"vcengine-attempt-lock");
	pthread_mutex_lock(&engine_mtx[veid]);


	/* setup stacks and ucontexts */

	char* vc_stack_base 
		= (char*) (((intptr_t)vc_stack_storage+VCORE_STACK_SZ)&VCORE_STACK_MASK);

	char* vc_stack[VCORE_NC];

	DEBUG(__FILE__,__LINE__,"vc_stack_base %p\n",vc_stack_base);

	for(i=0;i<VCORE_NC;i++) {
	
		int vcid = i + VCORE_NC*veid;	
		vc_stack[i] = vc_stack_base + i*VCORE_STACK_SZ;
		struct vc_data* data = (struct vc_data*)vc_stack[i];

#ifndef XCL_NO_UCONTEXT
		*data = (struct vc_data){vcid,&vc_uctx[i],0,&work};
		getcontext(&vc_uctx[i]);
		vc_uctx[i].uc_link = &vcengine_uctx;
		vc_uctx[i].uc_stack.ss_sp = vc_stack[i] + sizeof(struct vc_data);
		vc_uctx[i].uc_stack.ss_size = VCORE_STACK_SZ - sizeof(struct vc_data);
#endif

	}

	engine_local_mem_free[veid] = engine_local_mem_base[veid] = ve_local_mem;
	engine_local_mem_sz[veid] = VCORE_LOCAL_MEM_SZ;
	

	DEBUG(__FILE__,__LINE__,"vcengine-set-ready");
	engine_ready[veid] = 1;


	int d,g;
	int nc,ng;
	int ii,gg;
	div_t qr;
	struct cmdcall_arg* cmd_argp;

	DEBUG(__FILE__,__LINE__,"ENTER BIG LOOP FOR VCENGINE\n");

	do {

		DEBUG(__FILE__,__LINE__,"vcengine-wait1");

		pthread_cond_wait(&engine_sig1[veid],&engine_mtx[veid]);

		if (cmd_argp = engine_cmd_argp[veid]) {

			/* propagate info so that vcore() can execute specified kernel */

			DEBUG(__FILE__,__LINE__,"vcengine[%d]: krn %p",veid,cmd_argp->k.krn);
			edata->funcp = cmd_argp->k.ksym;
			edata->callp = cmd_argp->k.kcall;
			edata->pr_arg_vec = cmd_argp->k.pr_arg_vec;

			work.tdim = cmd_argp->k.work_dim;
			nc = ng = 1;
			size_t gid_offset[3];
			size_t gtid_offset[3];
			for(d = 0; d<work.tdim; d++) {

				work.gtsz[d] = cmd_argp->k.global_work_size[d];
				work.ltsz[d] = cmd_argp->k.local_work_size[d];

				work.gsz[d] = work.gtsz[d]/work.ltsz[d];

				gtid_offset[d] = cmd_argp->k.global_work_offset[d];
				gid_offset[d] = gtid_offset[d]/work.ltsz[d];

				DEBUG(__FILE__,__LINE__,"vcengine[%d]: %d %d %d %d\n",
					veid,d,gtid_offset[d],work.gtsz[d],work.ltsz[d]);

				nc *= work.ltsz[d];
				ng *= work.gsz[d];
			} 

			DEBUG(__FILE__,__LINE__,"vcengine[%d]: nc=%d  ng=%d",veid,nc,ng);

			for(i=0;i<nc;i++) {
				struct vc_data* data = (struct vc_data*)vc_stack[i];
				data->next_uctx = &vc_uctx[(i+1)%nc];
				int ii = i;
				switch(work.tdim) {
					case 3:
						qr = div(ii,(work.ltsz[1]*work.ltsz[0]));
						data->ltid[2] = qr.quot;
						ii = qr.rem;
					case 2:
						qr = div(ii,work.ltsz[0]);
						data->ltid[1] = qr.quot;
						ii = qr.rem;
					case 1:
						data->ltid[0] = ii;
					default: break;
				}
			} 

			edata->vc_runc = 0;

			DEBUG(__FILE__,__LINE__,"vcengine[%d]: num_of_groups %d",veid,ng);

			for(g=0;g<ng;g++) { /* loop over thread groups */

				/* set gid[], gtid[] */

				gg = g;
				switch(work.tdim) {
					case 3:
						qr = div(gg,(work.gsz[1]*work.gsz[0]));
						work.gid[2] = qr.quot + gid_offset[2];
						work.gtid[2] = qr.quot*work.ltsz[2] + gtid_offset[2];
						gg = qr.rem;
					case 2:
						qr = div(gg,work.gsz[0]);
						work.gid[1] = qr.quot + gid_offset[1];
						work.gtid[1] = qr.quot*work.ltsz[1] + gtid_offset[1];
						gg = qr.rem;
					case 1:
						work.gid[0] = gg + gid_offset[0];
						work.gtid[0] = gg*work.ltsz[0] + gtid_offset[0];
					default: break;
				} 

	
				switch(work.tdim) {
					case 3:
						DEBUG(__FILE__,__LINE__,
							"vcengine[%d]: gid[]={%d,%d,%d} gtid[]={%d,%d,%d}",veid,
							work.gid[0],work.gid[1],work.gid[2],
							work.gtid[0],work.gtid[1],work.gtid[2]);
						break;
					case 2:
						DEBUG(__FILE__,__LINE__,
							"vcengine[%d]: gid[]={%d,%d} gtid[]={%d,%d}",veid,
							work.gid[0],work.gid[1],
							work.gtid[0],work.gtid[1]);
						break;
					case 1:
						DEBUG(__FILE__,__LINE__,
							"vcengine[%d]: gid[]={%d} gtid[]={%d}",veid,
							work.gid[0],
							work.gtid[0]);
						break;
					default: break;
				} 

				DEBUG(__FILE__,__LINE__,"launching vcores");

#ifndef XCL_NO_UCONTEXT

				for(i=0;i<nc;i++)  
					makecontext(&vc_uctx[i],(void(*)(void))edata->callp,1,edata);

				for(i=0;i<nc;i++) {

					swapcontext(&vcengine_uctx,&vc_uctx[i]);

				} 
#endif

				if (edata->vc_runc) {
					DEBUG(__FILE__,__LINE__,"vcengine[%d]: unterminated vcore",veid);
					exit(-1);
				} else {
					DEBUG(__FILE__,__LINE__,
						"vcengine[%d]: all vcores completed",veid);
				} 

			} 

			engine_cmd_argp[veid] = 0;
			
		} 

		DEBUG(__FILE__,__LINE__,"vcengine-sig2");
		pthread_cond_signal(&engine_sig2[veid]);

	} while(!engine_shutdown[veid]);

	engine_shutdown[veid] = 2;

	pthread_mutex_unlock(&engine_mtx[veid]);
	DEBUG(__FILE__,__LINE__,"vcengine-unlock");

}


void* 
vcproc_startup( void* p )
{
	DEBUG(__FILE__,__LINE__,"vcproc_create");

	int i;

	pthread_attr_t td_attr;

	pthread_attr_init(&td_attr);
	pthread_attr_setdetachstate(&td_attr,PTHREAD_CREATE_JOINABLE);

	for(i=0;i<VCORE_NE;i++) {

		pthread_mutex_init(&engine_mtx[i],0);
		pthread_cond_init(&engine_sig1[i],0);
		pthread_cond_init(&engine_sig2[i],0);

		engine_ready[i] = 0;
		engine_shutdown[i] = 0;
		engine_cmd_argp[i] = 0;
		engine_data[i].veid = i;
		engine_data[i].vc_runc = 0;

		pthread_create(&engine_td[i],&td_attr,vcengine,
			(void*)&engine_data[i]);

		pthread_yield();

		while(!engine_ready[i]) pthread_yield();

		DEBUG(__FILE__,__LINE__,"ready[%d] %d",i,engine_ready[i]);
	}

	pthread_attr_destroy(&td_attr);

	return(0);
}



void*
vcproc_cmd( struct cmdcall_arg* argp)
{
	int i,j;
	int e;
	struct cmdcall_arg subcmd_argp[VCORE_NE];


	for(i=0;i<VCORE_NE;i++) {

		/* must spin until engine is read to ensure valid local cache is set */

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-spin-until-ready",i);
		while(!engine_ready[i]) pthread_yield(); 

	}


	/* first apply correction to global ptrs */

	DEBUG(__FILE__,__LINE__,"cmdcall_x86_64:ndrange_kernel: fix global ptrs %p",
		argp->k.arg_kind);

	for(i=0;i<argp->k.krn->narg;i++) {

		DEBUG(__FILE__,__LINE__, "fix global ptrs %d",i);

		DEBUG(__FILE__,__LINE__,"arg_kind=%d", argp->k.arg_kind[i]);

		switch(argp->k.arg_kind[i]) {

			case CLARG_KIND_GLOBAL:

				DEBUG(__FILE__,__LINE__, "argp->k.pr_arg_vec[i]=%p",
					argp->k.pr_arg_vec[i]);

				DEBUG(__FILE__,__LINE__, "*cl_mem=%p",
					(*(cl_mem*)argp->k.pr_arg_vec[i]));

				*(void**)argp->k.pr_arg_vec[i]
					=(*(cl_mem*)argp->k.pr_arg_vec[i])->host_ptr;

				break;

//			case CLARG_KIND_UNDEFINED:
//			case CLARG_KIND_VOID:
//			case CLARG_KIND_DATA:
//			case CLARG_KIND_LOCAL:
//			case CLARG_KIND_CONSTANT:
//			case CLARG_KIND_SAMPLER:
//			case CLARG_KIND_IMAGE2D:
//			case CLARG_KIND_IMAGE3D:

			default: break;
		}
	}


	/* make copies of *argp for each engine and allocate local mem */

	for(i=0;i<VCORE_NE;i++) {

		memcpy(subcmd_argp+i,argp,sizeof(struct cmdcall_arg));

		__clone(subcmd_argp[i].k.pr_arg_vec,argp->k.pr_arg_vec,
			argp->k.narg,void*);

   	__clone(subcmd_argp[i].k.pr_arg_buf,argp->k.pr_arg_buf,
			argp->k.arg_buf_sz,void);

		DEBUG(__FILE__,__LINE__,"ve[%d] arg_buf %p %p",
			i,argp->k.pr_arg_buf,subcmd_argp[i].k.pr_arg_buf);

   	intptr_t offset
      	= (intptr_t)subcmd_argp[i].k.pr_arg_buf 
			- (intptr_t)argp->k.pr_arg_buf;

		DEBUG(__FILE__,__LINE__,"ve[%d] pr_arg_buf offset %p",i,offset);

   	for(j=0;j<subcmd_argp[i].k.narg;j++) 
			DEBUG(__FILE__,__LINE__,"ve[%d] arg_vec[%d] %p",
				i,j,subcmd_argp[i].k.pr_arg_vec[j]);

   	for(j=0;j<subcmd_argp[i].k.narg;j++) 
      	subcmd_argp[i].k.pr_arg_vec[j] += offset;

   	for(j=0;j<subcmd_argp[i].k.narg;j++) 
			DEBUG(__FILE__,__LINE__,"ve[%d] +offset arg_vec[%d] %p",
				i,j,subcmd_argp[i].k.pr_arg_vec[j]);

	}


	size_t sz;

	for(e=0;e<VCORE_NE;e++) {

		for(i=0;i<subcmd_argp[e].k.krn->narg;i++) {

			switch(subcmd_argp[e].k.arg_kind[i]) {

				case CLARG_KIND_LOCAL:

					sz = *(size_t*)subcmd_argp[e].k.pr_arg_vec[i];

					if (engine_local_mem_sz[e] < sz) {
						ERROR(__FILE__,__LINE__,"out of local mem");
						return((void*)-1);						
					}

					DEBUG(__FILE__,__LINE__,"ve[%d] argn %d alloc local mem %p %d",
						e,i,engine_local_mem_free[e],sz);

					*(void**)subcmd_argp[e].k.pr_arg_vec[i] 
						= (void*)engine_local_mem_free[e];
					
					engine_local_mem_free[e] += sz;
					engine_local_mem_sz[e] -= sz;

					break;

//				case CLARG_KIND_UNDEFINED:
//				case CLARG_KIND_VOID:
//				case CLARG_KIND_DATA:
//				case CLARG_KIND_GLOBAL:
//				case CLARG_KIND_CONSTANT:
//				case CLARG_KIND_SAMPLER:
//				case CLARG_KIND_IMAGE2D:
//				case CLARG_KIND_IMAGE3D:

				default: break;
			}
		}

	}


	/* now modify them to partition the work across engines */
	/* XXX this is the simplest distribution possible, elaborate later -DAR */	
	unsigned int d = argp->k.work_dim;
	unsigned int gwsd1 = argp->k.global_work_size[d-1];
	unsigned int lwsd1 = argp->k.local_work_size[d-1];
	unsigned int ng = gwsd1/lwsd1;
	unsigned int nge = ng/VCORE_NE;
	unsigned int nge0 = nge + ng%VCORE_NE;	
	unsigned int inc0 = nge0*lwsd1;
	unsigned int inc = nge*lwsd1;

	DEBUG(__FILE__,__LINE__,"partitioning: %d %d -> %d %d \n",nge,nge0,inc,inc0);

	unsigned int gwo = argp->k.global_work_offset[d-1];

	DEBUG(__FILE__,__LINE__,"initial gwo %d\n",gwo);

	subcmd_argp[0].k.global_work_offset[d-1] = gwo;
	gwo += subcmd_argp[0].k.global_work_size[d-1] = inc0;

	for(i=1;i<VCORE_NE;i++) {
		subcmd_argp[i].k.global_work_offset[d-1] = gwo;
		gwo += subcmd_argp[i].k.global_work_size[d-1] = inc;
	}

	for(i=0;i<VCORE_NE;i++) 
		DEBUG(__FILE__,__LINE__,"%d %d %d\n",
			subcmd_argp[i].k.global_work_offset[d-1],
			subcmd_argp[i].k.global_work_size[d-1],
			subcmd_argp[i].k.local_work_size[d-1]);
	

	for(i=0;i<VCORE_NE;i++) {

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-spin-until-ready",i);
		while(!engine_ready[i]) pthread_yield(); 

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-attempt-lock",i);
		pthread_mutex_lock(&engine_mtx[i]);

		DEBUG(__FILE__,__LINE__,"ve[%d] arg0[2] %d",
			i,((int*)((subcmd_argp+i)->k.pr_arg_vec))[2]);

		engine_cmd_argp[i] = subcmd_argp+i;

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-sig1",i);
		pthread_cond_signal(&engine_sig1[i]);

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-unlock",i);
		pthread_mutex_unlock(&engine_mtx[i]);

	}


	pthread_yield(); 


	for(e=0;e<VCORE_NE;e++) {

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-attempt-lock",e);
		pthread_mutex_lock(&engine_mtx[e]);

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-wait2",e);
		if (engine_cmd_argp[e]) pthread_cond_wait(&engine_sig2[e],&engine_mtx[e]);

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd complete",e);

		engine_local_mem_free[e] = engine_local_mem_base[e];
		engine_local_mem_sz[e] = VCORE_LOCAL_MEM_SZ;

		DEBUG(__FILE__,__LINE__,"ve[%d] vcproc_cmd-unlock",e);
		pthread_mutex_unlock(&engine_mtx[e]);

	}

}


/* This should never be needed since the vcengines should be idle at the
 * time the program is being taken down.  If this is not true, the user
 * is exiting with uncompleted events in the queue and this should be
 * dealt with elsewhere.  This is left here for testing purposes. -DAR 

static void __attribute__((destructor))
__vcproc_shutdown_dtor( void* p )
{
	DEBUG(__FILE__,__LINE__,"__vcproc_shutdown_dtor");

	int i;
	int retval;
	int count;
	int remaining = VCORE_NE;

	while (remaining) for(i=0;i<VCORE_NE;i++) {

		DEBUG(__FILE__,__LINE__,"remaining %d",remaining);

		if (engine_data[i].vc_runc > 0) {
			fprintf(stderr,"very bad, vc_runc[%d] > 0",i);
			DEBUG(__FILE__,__LINE__,"pthread_cancel %d",i)
			pthread_cancel(engine_td[i]);
			DEBUG(__FILE__,__LINE__,"pthread_cancel succeded %d",i)
			engine_shutdown[i] = 3;
			--remaining;
		}

		DEBUG(__FILE__,__LINE__,"shutdown[%d] %d",i,engine_shutdown[i]);

		switch(engine_shutdown[i]) {

			case 0:
				pthread_mutex_lock(&engine_mtx[i]);
				engine_shutdown[i] = 1;
				pthread_cond_signal(&engine_sig[i]);
				pthread_mutex_unlock(&engine_mtx[i]);
				break;

			case 1:
				pthread_yield();
				break;

			case 2:

				pthread_join(engine_td[i],(void**)&retval);
				pthread_cond_destroy(&engine_sig[i]);
				pthread_mutex_destroy(&engine_mtx[i]);
				--remaining;
	
			case 3:
			default: break;

		}

		pthread_yield();

	}

	return;
}
*/


